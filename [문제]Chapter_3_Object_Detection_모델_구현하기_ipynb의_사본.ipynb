{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eTnghmuFBrEQ"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haewonChun/python_algorithms/blob/main/%5B%EB%AC%B8%EC%A0%9C%5DChapter_3_Object_Detection_%EB%AA%A8%EB%8D%B8_%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCa4_7v8fnkY"
      },
      "source": [
        "# 주제: Object Detection Model 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTnghmuFBrEQ"
      },
      "source": [
        "## 실습 가이드\n",
        "    1. 데이터를 다운로드하여 Colab에 불러옵니다.\n",
        "    2. 필요한 라이브러리는 import 가능합니다.\n",
        "    3. 코드는 위에서부터 아래로 순서대로 실행합니다.\n",
        "\n",
        "## 데이터 소개\n",
        "    - 이번 주제는 MS-COCO 2017 dataset을 사용합니다.\n",
        "    - 이번에 사용하는 MS-COCO dataset은 tfrecord 형태로 저장되어 있으며, 내부에 다양한 정보를 담고 있습니다.\n",
        "      그 중에 이번 주제와 관련된 내용들은 다음과 같습니다.\n",
        "\n",
        "    1. image\n",
        "      - 각 image의 pixel 값(3차원 tensor)\n",
        "      - image/filename: image file의 이름\n",
        "      - image/id: image file의 id(file명에서 확장자 제외한 부분)      \n",
        "      \n",
        "    2. objects\n",
        "      - objects/area: 각 bounding box들의 면적\n",
        "      - objects/id: 각 bounding box의 id\n",
        "      - objects/bbox: 각 bounding box의 좌표(ymax, xmax, ymin, xmin)\n",
        "      - objects/label: 각 bounding box에 대한 classification 정답\n",
        "\n",
        "- 원본 데이터 출처: https://cocodataset.org/#home\n",
        "- data format 참고: https://cocodataset.org/#format-data\n",
        "\n",
        "## 문제 소개\n",
        "    - 이번 실습에서는 1-stage object detection model인 RetinaNet을 직접 만들고 학습시켜보도록 하겠습니다.\n",
        "\n",
        "## RetinaNet\n",
        "    - Paper: https://arxiv.org/abs/1708.02002\n",
        "    - RetinaNet에 대한 강의가 따로 준비되어 있으니 강의를 시청하시고 문제를 풀어보시기 바랍니다.\n",
        "\n",
        "## 최종 목표    \n",
        "    - Object Detection Model에 대한 이해\n",
        "    - Object Detection Model 구현\n",
        "\n",
        "- 출제자: 이진원 강사"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WdJtwIxbcO8"
      },
      "source": [
        "## Step 1. Data 다운로드 및 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvqJ-wnC9LZq"
      },
      "source": [
        "## library를 import 합니다\n",
        "## 추가로 필요한 library가 있으면 추가로 import 해도 좋습니다\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FsaeppHcDsw"
      },
      "source": [
        "## Hyper parameter 및 기타 설정\n",
        "num_classes = 80\n",
        "batch_size = 2\n",
        "\n",
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")\n",
        "\n",
        "## ckpt 저장할 directory\n",
        "model_dir = \"retinanet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwb0M49Fi8w"
      },
      "source": [
        "### 문제 1. Data 불러오기\n",
        "\n",
        "    - data는 아래 url(google drive)에 저장되어 있습니다.(zip 파일)\n",
        "    - gdown library를 이용하여 data를 다운받고, zip파일 압축을 풀어줍니다.\n",
        "    - 압축 파일은 data라는 이름의 directory에 풀도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgENz8HQxTD0"
      },
      "source": [
        "url = \"https://drive.google.com/uc?id=1vIHxg4fLsK7Vn1NnZ_toK4akqpT5ZoKM\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YICXwtev616c"
      },
      "source": [
        "## data download 받고(gdown.download 사용) 압축 풀기\n",
        "##### CODE HERE #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03nOhKLw63-a"
      },
      "source": [
        "    - 위에서 다운받은 파일은 실제 MS-COCO data의 일부만을 포함하고 있습니다.\n",
        "    - 아래의 cell을 실행하면 이 data를 이용하여 dataset을 만들 수 있습니다.\n",
        "    - MS-COCO data 전체가 필요한 경우 아래 data_dir='data' 부분을 data_dir=None 으로 변경하면 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wGbCLUAGNSS"
      },
      "source": [
        "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
        "    \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir='data'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhb3IOgb7ZtL"
      },
      "source": [
        "    - dataset_info를 통해서 data에 관한 어떤 정보들이 있는지 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7SQC6bUOD2n"
      },
      "source": [
        "dataset_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSISq8aI7tc5"
      },
      "source": [
        "    - tfds.show_examples를 활용하여 data에 포함된 image를 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXRH5t_KNWL3"
      },
      "source": [
        "tfds.show_examples(train_dataset, dataset_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcXVrsuE70dy"
      },
      "source": [
        "### 문제 2. Dataset 내부 item 확인\n",
        "    - train_dataset에서 data를 하나 꺼내서 내부 item을 print 문으로 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7BHYq6p7aYJ"
      },
      "source": [
        "##### CODE HERE #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHxdSRUF81cS"
      },
      "source": [
        "### 문제 3. Data 직접 확인하기\n",
        "    - 문제 2와 같이 dataset에서 1개의 data를 가져온 후,\n",
        "      image와 bounding box를 화면에 함께 출력해봅시다.\n",
        "    - data에 저장된 bounding box의 좌표는 box의 양쪽 모서리의 좌표이며,\n",
        "      (ymin, xmin, ymax, xmax)의 순서로 되어 있습니다.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY7ewiJF-9TV"
      },
      "source": [
        "for data in train_dataset.take(1):\n",
        "  image = np.array(data['image'], dtype=np.uint8)\n",
        "  plt.figure(figsize=(8,8))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  ax = plt.gca()\n",
        "  boxes = data['objects']['bbox']\n",
        "\n",
        "  ##### CODE HERE #####\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HneT9jfgjlPC"
      },
      "source": [
        "## Step 2. Data Augmentation\n",
        "이번 Step에서는 data augmentation을 구현해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyN5GIfPFJfQ"
      },
      "source": [
        "    - 행렬과 같은 tensor의 경우 행->열 순서로 되어 있기 때문에 (y좌표, x좌표) 순서로 저장되어 있습니다.\n",
        "    - (x좌표, y좌표) 순서가 더 직관적으로 이해하기 쉬우므로 저장된 x, y좌표의 순서를 바꿔주는 함수를 만들어봅시다.\n",
        "    - 함수의 입력은 (N, 4) shape의 bounding box 정보이며,\n",
        "      bounding box 좌표 정보는 양쪽 모서리의 x, y 좌표값으로 되어 있다고 가정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7WtjHbqztZ7"
      },
      "source": [
        "def swap_xy(boxes):\n",
        "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-06hjOzF-OH"
      },
      "source": [
        "### 문제 4. Bounding box format 변경하는 함수 만들기 1\n",
        "    - bounding box의 위치 정보에 대한 format을 변경하는 함수를 만들어 봅시다.\n",
        "    - (xmin, ymin, xmax, ymax) 형태의 bounding box format을 (x, y, w, h)로 바꿔서 반환합니다.\n",
        "    - 이 때 x, y는 bounding box의 center 좌표를 의미합니다.\n",
        "    - 함수의 입력은 (..., N, 4)와 같이 2차원 이상의 tensor입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV_40s1azwcS"
      },
      "source": [
        "def convert_to_xywh(boxes):\n",
        "    ##### CODE HERE #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def xyxy_to_xywh(boxes):\n",
        "    \"\"\"\n",
        "    입력으로 받은 bounding box 좌표를 (x, y, w, h) 형태로 변경하는 함수.\n",
        "\n",
        "    Args:\n",
        "        boxes (torch.Tensor): bounding box 좌표를 포함한 텐서. (..., N, 4) 형태여야 함.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 변경된 bounding box 좌표를 포함한 텐서. (..., N, 4) 형태.\n",
        "    \"\"\"\n",
        "    # bounding box의 xmin, ymin, xmax, ymax를 추출\n",
        "    xmin = boxes[..., 0:1]\n",
        "    ymin = boxes[..., 1:2]\n",
        "    xmax = boxes[..., 2:3]\n",
        "    ymax = boxes[..., 3:4]\n",
        "\n",
        "    # bounding box의 중심 좌표 (x, y) 계산\n",
        "    x = (xmin + xmax) / 2\n",
        "    y = (ymin + ymax) / 2\n",
        "\n",
        "    # bounding box의 너비 (w)와 높이 (h) 계산\n",
        "    w = xmax - xmin\n",
        "    h = ymax - ymin\n",
        "\n",
        "    # 변경된 (x, y, w, h) 형태의 bounding box 좌표를 반환\n",
        "    return torch.cat((x, y, w, h), dim=-1)\n",
        "\n",
        "# 예제 사용법\n",
        "# (xmin, ymin, xmax, ymax) 형태의 bounding box 좌표를 가진 텐서 생성\n",
        "bounding_boxes = torch.tensor([[[0, 1, 3, 4], [2, 2, 5, 6]]], dtype=torch.float32)\n",
        "\n",
        "# 함수를 사용하여 좌표를 변경\n",
        "new_boxes = xyxy_to_xywh(bounding_boxes)\n",
        "\n",
        "# 변경된 좌표 출력\n",
        "print(new_boxes)\n"
      ],
      "metadata": {
        "id": "wJYMWkYIq22p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-c9LiCHHptf"
      },
      "source": [
        "### 문제 5. Bounding box format 변경하는 함수 만들기 2\n",
        "    - bounding box의 위치 정보에 대한 format을 변경하는 함수를 만들어 봅시다.\n",
        "    - (x, y, w, h) 형태의 bounding box format을 (xmin, ymin, xmax, ymax)로 바꿔서 반환합니다.\n",
        "    - 이 때 x, y는 bounding box의 center 좌표를 의미합니다.\n",
        "    - 함수의 입력은 (..., N, 4)와 같이 2차원 이상의 tensor입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CewGC3MBzwfE"
      },
      "source": [
        "def convert_to_corners(boxes):\n",
        "    ##### CODE HERE #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def xywh_to_xyxy(boxes):\n",
        "    \"\"\"\n",
        "    입력으로 받은 bounding box 좌표를 (xmin, ymin, xmax, ymax) 형태로 변경하는 함수.\n",
        "\n",
        "    Args:\n",
        "        boxes (torch.Tensor): bounding box 좌표를 포함한 텐서. (..., N, 4) 형태여야 함.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 변경된 bounding box 좌표를 포함한 텐서. (..., N, 4) 형태.\n",
        "    \"\"\"\n",
        "    # bounding box의 중심 좌표 (x, y) 계산\n",
        "    x = boxes[..., 0:1]\n",
        "    y = boxes[..., 1:2]\n",
        "\n",
        "    # bounding box의 너비 (w)와 높이 (h) 계산\n",
        "    w = boxes[..., 2:3]\n",
        "    h = boxes[..., 3:4]\n",
        "\n",
        "    # (xmin, ymin, xmax, ymax) 형태로 변경\n",
        "    xmin = x - (w / 2)\n",
        "    ymin = y - (h / 2)\n",
        "    xmax = x + (w / 2)\n",
        "    ymax = y + (h / 2)\n",
        "\n",
        "    # 변경된 (xmin, ymin, xmax, ymax) 형태의 bounding box 좌표를 반환\n",
        "    return torch.cat((xmin, ymin, xmax, ymax), dim=-1)\n",
        "\n",
        "# 예제 사용법\n",
        "# (x, y, w, h) 형태의 bounding box 좌표를 가진 텐서 생성\n",
        "bounding_boxes = torch.tensor([[[1, 2, 2, 3], [3, 4, 2, 2]]], dtype=torch.float32)\n",
        "\n",
        "# 함수를 사용하여 좌표를 변경\n",
        "new_boxes = xywh_to_xyxy(bounding_boxes)\n",
        "\n",
        "# 변경된 좌표 출력\n",
        "print(new_boxes)\n"
      ],
      "metadata": {
        "id": "axaiAB_grGJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP-a-RIL_G5X"
      },
      "source": [
        "### 문제 6. Image resizing 함수 만들기\n",
        "    - 짧은 변을 min_side와 같게 resize 합니다\n",
        "    - 만약에 긴 변의 길이가 max_side보다 클 경우에는 긴 변이 max_side와 같아지도록 다시 resize 합니다\n",
        "    - image size(가로, 세로 모두)가 stride의 배수가 아닐 경우 stride의 배수가 되도록 오른쪽과 아래쪽에 0을 채웁니다(zero padding)\n",
        "    - 함수의 입력값은 다음과 같습니다.\n",
        "      1. image: 3차원 tensor로 이루어진 image(feature map)의 pixel 값(height, width, channel)\n",
        "      2. min_side: resize에 사용할 짧은 변 길이\n",
        "      3. max_side: resize에 사용할 긴 변 길이\n",
        "      4. stride: 1번 입력인 image(feature map)의 1 pixel이 실제 원본 image에서 몇 pixel에 해당되는지\n",
        "    - 함수의 반환값은 3가지이고 각각 다음과 같습니다.\n",
        "      1. resize 및 padding된 image(feature map)의 pixel 값\n",
        "      2. padding 하기 전의 image size\n",
        "      3. padding 하기 전 image와 원본 image의 확대/축소 비율(resize 후/resize 전)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slp75Jwb_GKF"
      },
      "source": [
        "def resize_and_pad_image(image, min_side=800.0, max_side=1333.0, stride=128.0):\n",
        "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
        "    ratio = min_side / tf.reduce_min(image_shape)\n",
        "\n",
        "    ##### CODE HERE #####\n",
        "\n",
        "    return image, image_shape, ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "def resize_and_pad_image(image, min_side, max_side, stride):\n",
        "    \"\"\"\n",
        "    이미지를 resize하고 padding하여 새로운 이미지와 관련된 정보를 반환하는 함수.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): 3차원 텐서로 이루어진 이미지(feature map)의 pixel 값 (height, width, channel).\n",
        "        min_side (int): 짧은 변의 길이로 resize에 사용할 값.\n",
        "        max_side (int): 긴 변의 길이로 resize에 사용할 값.\n",
        "        stride (int): 입력 이미지의 1 pixel이 실제 원본 이미지에서 몇 pixel에 해당되는지.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: resize 및 padding된 이미지(feature map)의 pixel 값.\n",
        "        tuple: padding 하기 전의 원본 이미지 크기 (height, width).\n",
        "        tuple: padding 하기 전 이미지와 원본 이미지의 확대/축소 비율 (resize 후/resize 전).\n",
        "    \"\"\"\n",
        "    # 원본 이미지 크기\n",
        "    original_height, original_width = image.shape[0], image.shape[1]\n",
        "\n",
        "    # 짧은 변을 min_side와 같게 resize\n",
        "    scale = min_side / min(original_height, original_width)\n",
        "    new_height = int(round(original_height * scale))\n",
        "    new_width = int(round(original_width * scale))\n",
        "    image = torch.nn.functional.interpolate(image.unsqueeze(0), size=(new_height, new_width), mode='bilinear', align_corners=False)\n",
        "    image = image.squeeze(0)\n",
        "\n",
        "    # 만약에 긴 변의 길이가 max_side보다 클 경우 긴 변이 max_side와 같아지도록 다시 resize\n",
        "    if max(new_height, new_width) > max_side:\n",
        "        scale = max_side / max(new_height, new_width)\n",
        "        new_height = int(round(new_height * scale))\n",
        "        new_width = int(round(new_width * scale))\n",
        "        image = torch.nn.functional.interpolate(image.unsqueeze(0), size=(new_height, new_width), mode='bilinear', align_corners=False)\n",
        "        image = image.squeeze(0)\n",
        "\n",
        "    # image size(가로, 세로 모두)가 stride의 배수가 아닐 경우 padding\n",
        "    pad_height = int(math.ceil(new_height / stride)) * stride\n",
        "    pad_width = int(math.ceil(new_width / stride)) * stride\n",
        "    padding_height = pad_height - new_height\n",
        "    padding_width = pad_width - new_width\n",
        "\n",
        "    # 오른쪽과 아래쪽에 0을 채움\n",
        "    image = torch.nn.functional.pad(image, (0, padding_width, 0, padding_height), mode='constant', value=0)\n",
        "\n",
        "    # padding 하기 전의 원본 이미지 크기와 확대/축소 비율 계산\n",
        "    original_size = (original_height, original_width)\n",
        "    scale_factors = (pad_height / original_height, pad_width / original_width)\n",
        "\n",
        "    return image, original_size, scale_factors\n",
        "\n",
        "# 예제 사용법\n",
        "# 이미지를 3차원 텐서로 나타낸다고 가정하고 이를 입력으로 사용\n",
        "image = torch.randn((256, 384, 3))  # 예제 이미지 (높이: 256, 너비: 384, 채널: 3)\n",
        "\n",
        "min_side = 200\n",
        "max_side = 400\n",
        "stride = 32\n",
        "\n",
        "resized_image, original_size, scale_factors = resize_and_pad_image(image, min_side, max_side, stride)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Resize 및 Padding된 이미지 크기:\", resized_image.shape)\n",
        "print(\"원본 이미지 크기:\", original_size)\n",
        "print(\"확대/축소 비율 (resize 후/resize 전):\", scale_factors)\n"
      ],
      "metadata": {
        "id": "36YY7yHFrO4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIcF3LCHArQ9"
      },
      "source": [
        "### 문제 7. Horizontal flip 함수 만들기\n",
        "    - 50%의 확률로 image를 좌우반전합니다\n",
        "    - 이 때 bounding box의 좌표도 좌우반전에 맞게 변경합니다\n",
        "    - 입력 image는 3차원 tensor로 (height, width, channel)로 구성되어 있습니다.\n",
        "    - bounding box의 좌표는 (xmin, ymin, xmax, ymax)로 구성되어 있고, 각각의 좌표는 0~1 사이 값으로 normalized 되어 있다고 가정합니다.\n",
        "    - 함수의 반환값은 image의 pixel 값, bounding box의 좌표로 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA4-ihsj9Uwb"
      },
      "source": [
        "def random_flip_horizontal(image, boxes):\n",
        "    ##### CODE HERE #####\n",
        "    return image, boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "def horizontal_flip(image, boxes):\n",
        "    \"\"\"\n",
        "    입력 이미지와 bounding box 좌표를 좌우반전하는 함수.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): 3차원 텐서로 이루어진 이미지(pixel 값) (height, width, channel).\n",
        "        boxes (torch.Tensor): bounding box 좌표를 포함한 텐서 (N, 4), 각각의 좌표는 0~1 사이 값으로 normalized 됨.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 좌우반전된 이미지(pixel 값).\n",
        "        torch.Tensor: 좌우반전된 bounding box 좌표.\n",
        "    \"\"\"\n",
        "    if random.random() < 0.5:\n",
        "        # 이미지 좌우반전\n",
        "        image = torch.flip(image, [-1])\n",
        "\n",
        "        # bounding box 좌표도 좌우반전에 맞게 변경\n",
        "        xmin, ymin, xmax, ymax = torch.chunk(boxes, 4, dim=1)\n",
        "        new_xmin = 1.0 - xmax\n",
        "        new_xmax = 1.0 - xmin\n",
        "        boxes = torch.cat((new_xmin, ymin, new_xmax, ymax), dim=1)\n",
        "\n",
        "    return image, boxes\n",
        "\n",
        "# 예제 사용법\n",
        "# 이미지를 3차원 텐서로 나타낸다고 가정하고 이를 입력으로 사용\n",
        "image = torch.randn((256, 384, 3))  # 예제 이미지 (높이: 256, 너비: 384, 채널: 3)\n",
        "\n",
        "# bounding box 좌표 (xmin, ymin, xmax, ymax)를 포함한 텐서 (0~1 사이 값으로 normalized)\n",
        "boxes = torch.tensor([[0.1, 0.2, 0.4, 0.6], [0.3, 0.4, 0.6, 0.7]], dtype=torch.float32)\n",
        "\n",
        "flipped_image, flipped_boxes = horizontal_flip(image, boxes)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"좌우반전된 이미지 크기:\", flipped_image.shape)\n",
        "print(\"좌우반전된 bounding box 좌표:\", flipped_boxes)\n"
      ],
      "metadata": {
        "id": "7rq_FxilrYif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iisXvZbB4YF"
      },
      "source": [
        "### 문제 8. Augmentation 함수 만들기\n",
        "    - tf.data(dataset)에 map으로 적용할 수 있도록 함수를 만듭니다.\n",
        "    - 입력으로 dataset의 item을 받습니다\n",
        "    - 위에서 작성한 random_flip_horizontal 함수를 먼저 적용하고, 다음으로 resize_and_pad_image 함수를 적용합니다.\n",
        "    - 0~1 사이 값으로 normailized된 bounding box 좌표를 실제 image size에 맞게 조정합니다.\n",
        "    - bounding box 좌표를 (xmin, ymin, xmax, ymax) 형태에서 (x, y, w, h) 형태로 변경합니다.\n",
        "    - 함수의 반환값은 image의 pixel 값, bounding box의 좌표, class id로 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "608byiMxENdV"
      },
      "source": [
        "def preprocess_data(sample):\n",
        "    image = sample[\"image\"]\n",
        "    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
        "    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
        "\n",
        "    ##### CODE HERE #####\n",
        "\n",
        "    return image, bbox, class_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def process_dataset_item(item, min_side, max_side, stride):\n",
        "    \"\"\"\n",
        "    dataset의 item을 받아 처리하고 이미지와 bounding box를 반환하는 함수.\n",
        "\n",
        "    Args:\n",
        "        item (tuple): dataset에서 반환되는 item으로 (image, boxes, class_id) 형태로 구성됨.\n",
        "        min_side (int): resize에 사용할 짧은 변의 길이.\n",
        "        max_side (int): resize에 사용할 긴 변의 길이.\n",
        "        stride (int): 입력 이미지의 1 pixel이 실제 원본 이미지에서 몇 pixel에 해당되는지.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 처리된 이미지(pixel 값).\n",
        "        torch.Tensor: 처리된 bounding box 좌표 (x, y, w, h).\n",
        "        int: class id.\n",
        "    \"\"\"\n",
        "    image, boxes, class_id = item\n",
        "\n",
        "    # 이미지를 50% 확률로 좌우반전\n",
        "    image, boxes = horizontal_flip(image, boxes)\n",
        "\n",
        "    # resize 및 padding된 이미지 생성\n",
        "    resized_image, original_size, scale_factors = resize_and_pad_image(image, min_side, max_side, stride)\n",
        "\n",
        "    # normalized된 bounding box 좌표를 원본 이미지 크기에 맞게 조정\n",
        "    boxes[:, 0::2] *= original_size[1]  # x 좌표 조정\n",
        "    boxes[:, 1::2] *= original_size[0]  # y 좌표 조정\n",
        "\n",
        "    # bounding box 좌표를 (xmin, ymin, xmax, ymax) 형태에서 (x, y, w, h) 형태로 변경\n",
        "    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]  # w 계산\n",
        "    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]  # h 계산\n",
        "\n",
        "    return resized_image, boxes, class_id\n",
        "\n",
        "# 예제 사용법\n",
        "# item은 (image, boxes, class_id) 형태의 튜플로 가정합니다.\n",
        "item = (torch.randn((256, 384, 3)), torch.tensor([[0.1, 0.2, 0.4, 0.6], [0.3, 0.4, 0.6, 0.7]], dtype=torch.float32), 1)\n",
        "\n",
        "min_side = 200\n",
        "max_side = 400\n",
        "stride = 32\n",
        "\n",
        "processed_image, processed_boxes, class_id = process_dataset_item(item, min_side, max_side, stride)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"처리된 이미지 크기:\", processed_image.shape)\n",
        "print(\"처리된 bounding box 좌표:\", processed_boxes)\n",
        "print(\"class id:\", class_id)\n"
      ],
      "metadata": {
        "id": "O8ILqXtOreBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVuBmsUc3_GL"
      },
      "source": [
        "## Step 3. Anchor Box 정보 만들기\n",
        "AnchorBox class를 만들고, 모든 anchor box의 (x, y, w, h) 정보를 생성해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5d3mtdQUpUO"
      },
      "source": [
        "      * _compute_dims: 각 level별로 ahcnor box의 (w,h)를 계산하여 반환합니다.\n",
        "      * _get_anchors: 각 level별로 anchor box의 (x,y,w,h)를 계산하여 반환합니다. 이 때 return shape은 (height*width*9, 4)입니다.\n",
        "      * get_anchors: _get_anchors의 level별 anchor box정보를 모두 합쳐서 최종 결과를 반환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edpIGs9ET8j1"
      },
      "source": [
        "    - AnchorBox class를 만들고, get_anchors method를 call하면 모든 anchor box의 (x,y,w,h) 좌표를 반환하도록 합니다.    \n",
        "    - P3-P7까지의 모든 level의 anchor box의 (x,y,w,h) 좌표가 반환되어야 합니다.\n",
        "    - 계산의 편의를 위하여 먼저 각 level의 anchor box를 (height, width, 9, 4)의 shape을 갖는 tensor로 만듭니다.\n",
        "      여기서 height, width는 각 level의 feature map size를 의미하며, 9는 각 level의 anchor box 갯수이며 4는 (x,y,w,h)를 말합니다.\n",
        "    - 최종적으로 반환되는 값은 (5*height*width*9, 4)의 형태가 되며 맨 앞의 5는 level의 갯수(P3, P4, P5, P6, P7)을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class AnchorBox:\n",
        "    def __init__(self):\n",
        "        self.scales = [2 ** 0, 2 ** (1 / 3), 2 ** (2 / 3)]\n",
        "        self.ratios = [0.5, 1, 2]\n",
        "        self.base_sizes = [8, 16, 32, 64, 128]\n",
        "\n",
        "    def _compute_dims(self, level):\n",
        "        aspect_ratios = self.ratios\n",
        "        scales = self.scales\n",
        "        num_anchors = len(aspect_ratios) * len(scales)\n",
        "\n",
        "        anchor_dims = []\n",
        "        for scale in scales:\n",
        "            for ratio in aspect_ratios:\n",
        "                w = scale * ratio\n",
        "                h = scale / ratio\n",
        "                anchor_dims.append((w, h))\n",
        "\n",
        "        anchor_dims = torch.tensor(anchor_dims) * self.base_sizes[level]\n",
        "        return anchor_dims\n",
        "\n",
        "    def _get_anchors(self, level, feature_height, feature_width):\n",
        "        anchor_dims = self._compute_dims(level)\n",
        "        shift_x = torch.arange(0, feature_width) * self.base_sizes[level]\n",
        "        shift_y = torch.arange(0, feature_height) * self.base_sizes[level]\n",
        "        shift_x, shift_y = torch.meshgrid(shift_x, shift_y)\n",
        "\n",
        "        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=-1)\n",
        "        shifts = shifts.unsqueeze(2).expand(-1, -1, anchor_dims.shape[0], -1)\n",
        "\n",
        "        anchors = shifts + anchor_dims.view(1, 1, -1, 4)\n",
        "        return anchors.view(-1, 4)\n",
        "\n",
        "    def get_anchors(self, image_size):\n",
        "        anchors = []\n",
        "        for level, base_size in enumerate(self.base_sizes):\n",
        "            feature_height = image_size[0] // base_size\n",
        "            feature_width = image_size[1] // base_size\n",
        "            level_anchors = self._get_anchors(level, feature_height, feature_width)\n",
        "            anchors.append(level_anchors)\n",
        "\n",
        "        return torch.cat(anchors, dim=0)\n",
        "\n",
        "# 예제 사용법\n",
        "anchor_generator = AnchorBox()\n",
        "image_size = (800, 800)  # 이미지 크기 (높이, 너비)\n",
        "anchors = anchor_generator.get_anchors(image_size)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"전체 anchor box의 수:\", anchors.shape[0])\n",
        "print(\"각 anchor box의 (x, y, w, h) 좌표 예시:\")\n",
        "print(anchors[:10])  # 처음 10개의 anchor box 출력\n"
      ],
      "metadata": {
        "id": "s5xtHU8Xrsfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTHjUA6sVQZF"
      },
      "source": [
        "Class 내부의 method는 다음과 같습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef03QGGiUaUX"
      },
      "source": [
        "### \\_\\_init\\_\\_ method\n",
        "    - __init__ : anchor box 정보 계산을 위한 기본 값들 setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPiKO542Ur5t"
      },
      "source": [
        "### 문제 9. _compute_dims method\n",
        "    - _compute_dims method를 만들어봅시다.\n",
        "    - _compute_dims는 모든 level(P3-P7)에 대하여 각 level별로 anchor box의 (w, h)를 계산하여 반환하는 역할을 합니다.\n",
        "    - anchor_dims_all list에 해당 정보가 저장되며 list의 원소는 각각 (1,1,9,2)의 shape을 갖게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbIjuz9rVjGE"
      },
      "source": [
        "### 문제 10. _get_anchors method\n",
        "    - _get_anchors method를 만들어봅시다.\n",
        "    - _get_anchors method는 각 level별로 anchor box의 (x,y,w,h)를 계산하여 반환합니다.\n",
        "    - 반환값의 shape은 (height*width*9, 4)입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4VJ-ddeWE_e"
      },
      "source": [
        "### 문제 11. get_anchors method\n",
        "    - get_anchors method를 만들어봅시다.\n",
        "    - get_anchors method는 문제 9의 _get_anchors 를 통해서 level별 anchor box의 정보를 받아 이를 모두 합쳐서 최종 결과를 반환합니다.\n",
        "    - 반환값의 shape은 (5*height*width*9, 4)입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96lom4LRAuFA"
      },
      "source": [
        "class AnchorBox:\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "\n",
        "                ##### CODE HERE #####\n",
        "\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        anchors = [\n",
        "            ##### CODE HERE #####\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5svKDLTNUo1m"
      },
      "source": [
        "## Step 4. Label Encoding\n",
        "anchor box 정보와 정답(groundtruth) box 정보, class id를 이용하여 detection에서 사용할 label을 만들어봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buP69TdpWpPd"
      },
      "source": [
        "### 문제 12. IOU 계산 함수 만들기\n",
        "\n",
        "    - 이 함수는 2개의 bounding box 그룹들(boxes1, boxes2) 간에 iou를 계산합니다\n",
        "    - 모든 anchor box와 gound truth box 간의 iou를 계산하여,\n",
        "      anchor box들을 positive, negative, ignore로 구분하기 위해서 사용합니다\n",
        "    - 각 bounding box의 좌표 정보는 (x,y,w,h) 형태로 입력받습니다.\n",
        "    - boxes1의 shape은 (N, 4)이고 boxes2의 shape은 (M, 4)라고 할 때, 출력은 (N, M) shape의 tensor가 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBOaK35TNMkp"
      },
      "source": [
        "def compute_iou(boxes1, boxes2):\n",
        "    boxes1_corners = convert_to_corners(boxes1)\n",
        "    boxes2_corners = convert_to_corners(boxes2)\n",
        "\n",
        "    ##### CODE HERE #####\n",
        "\n",
        "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def calculate_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    두 개의 bounding box 그룹 간의 IoU(intersection over union)를 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        boxes1 (torch.Tensor): bounding box 그룹 1의 좌표 정보 (N, 4), N은 bounding box의 개수.\n",
        "        boxes2 (torch.Tensor): bounding box 그룹 2의 좌표 정보 (M, 4), M은 bounding box의 개수.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 각 조합에 대한 IoU 값 (N, M).\n",
        "    \"\"\"\n",
        "    # 각 bounding box 그룹의 좌표 정보를 (x1, y1, x2, y2) 형태로 변경\n",
        "    x1_1, y1_1, x2_1, y2_1 = boxes1[:, 0], boxes1[:, 1], boxes1[:, 0] + boxes1[:, 2], boxes1[:, 1] + boxes1[:, 3]\n",
        "    x1_2, y1_2, x2_2, y2_2 = boxes2[:, 0], boxes2[:, 1], boxes2[:, 0] + boxes2[:, 2], boxes2[:, 1] + boxes2[:, 3]\n",
        "\n",
        "    # IoU를 계산하기 위한 교차 영역 계산\n",
        "    x1_intersection = torch.max(x1_1.unsqueeze(1), x1_2)\n",
        "    y1_intersection = torch.max(y1_1.unsqueeze(1), y1_2)\n",
        "    x2_intersection = torch.min(x2_1.unsqueeze(1), x2_2)\n",
        "    y2_intersection = torch.min(y2_1.unsqueeze(1), y2_2)\n",
        "\n",
        "    # 교차 영역의 너비와 높이 계산 (clamp 함수를 사용하여 음수 값은 0으로 만듦)\n",
        "    intersection_width = torch.clamp(x2_intersection - x1_intersection, min=0)\n",
        "    intersection_height = torch.clamp(y2_intersection - y1_intersection, min=0)\n",
        "\n",
        "    # 교차 영역의 면적 계산\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "\n",
        "    # 각 bounding box의 면적 계산\n",
        "    area1 = boxes1[:, 2] * boxes1[:, 3]\n",
        "    area2 = boxes2[:, 2] * boxes2[:, 3]\n",
        "\n",
        "    # IoU 계산\n",
        "    iou = intersection_area / (area1.unsqueeze(1) + area2 - intersection_area)\n",
        "\n",
        "    return iou\n",
        "\n",
        "# 예제 사용법\n",
        "boxes1 = torch.tensor([[0, 0, 2, 2], [3, 3, 5, 5]], dtype=torch.float32)\n",
        "boxes2 = torch.tensor([[1, 1, 3, 3], [4, 4, 6, 6]], dtype=torch.float32)\n",
        "\n",
        "iou_matrix = calculate_iou(boxes1, boxes2)\n",
        "\n",
        "# 결과 출력\n",
        "print(iou_matrix)\n"
      ],
      "metadata": {
        "id": "MuUFzAiFr6TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdCyK1gWZy3-"
      },
      "source": [
        "### LabelEncoder Class\n",
        "이제 LabelEncoder class를 작성해보겠습니다.\n",
        "이 class에서는 앞에서 생성한 anchor box 정보와 groundtruth box 정보, class id를 이용하여 실제 학습에 사용할 label을 생성해줍니다.\n",
        "해당 class의 method는 다음과 같습니다.\n",
        "\n",
        "### \\_\\_init\\_\\_ method\n",
        "\n",
        "    - AnchorBox class instance 생성\n",
        "    - box normalize를 위한 variance\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHjAsAS5BwwT"
      },
      "source": [
        "### 문제 13. _match_anchor_boxes method\n",
        "    - IOU를 기반으로 gt box를 anchor box에 matching하는 역할을 합니다.\n",
        "    - M개의 anchor box와 N개의 gt box에 대하여 계산된 MxN의 iou matrix를 이용하여 각 행에서 IOU의 최대값을 찾습니다.\n",
        "    - 이 때 최대 IOU에 해당되는 index도 저장합니다.(matched_gt_index)\n",
        "    - 최대 IOU 값이 match_iou 이상이면 positive, ignore_iou 미만이면 negative, 나머지는 ignore로 처리하여\n",
        "      각 anchor box마다 positive, negative, ignore 여부를 알 수 있는 postive_mask, negative mask, ignore_mask를 생성합니다.\n",
        "    - 입력값은 anchor box의 (x,y,w,h)정보, gt box의 (x,y,w,h)정보, match_iou, ignore_iou 입니다.\n",
        "    - 반환값은 matched_gt_index, positive_mask, ignore_mask 입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class AnchorBox:\n",
        "    # AnchorBox 클래스의 나머지 부분은 이전 예제에서 사용한 코드를 그대로 사용합니다.\n",
        "\n",
        "def match_anchors_with_gt(anchors, gt_boxes, match_iou, ignore_iou):\n",
        "    \"\"\"\n",
        "    Anchor box와 GT box 간의 매칭을 수행하여 positive, negative, ignore 여부를 판단하고\n",
        "    matched_gt_index, positive_mask, ignore_mask를 반환하는 함수.\n",
        "\n",
        "    Args:\n",
        "        anchors (torch.Tensor): Anchor box의 (x, y, w, h) 좌표 정보 (M, 4), M은 Anchor box의 개수.\n",
        "        gt_boxes (torch.Tensor): GT box의 (x, y, w, h) 좌표 정보 (N, 4), N은 GT box의 개수.\n",
        "        match_iou (float): positive로 매칭할 IoU 임계값.\n",
        "        ignore_iou (float): ignore으로 처리할 IoU 임계값.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 매칭된 GT box의 인덱스 (M,).\n",
        "        torch.Tensor: positive mask (M,), positive로 매칭된 anchor는 True, 그렇지 않은 경우 False.\n",
        "        torch.Tensor: ignore mask (M,), ignore로 처리된 anchor는 True, 그렇지 않은 경우 False.\n",
        "    \"\"\"\n",
        "    iou_matrix = calculate_iou(anchors, gt_boxes)\n",
        "\n",
        "    # 각 anchor에 대해 최대 IoU와 그에 해당하는 GT box의 인덱스를 찾음\n",
        "    max_iou, matched_gt_index = iou_matrix.max(dim=1)\n",
        "\n",
        "    # positive, negative, ignore mask 생성\n",
        "    positive_mask = max_iou >= match_iou\n",
        "    negative_mask = max_iou < ignore_iou\n",
        "    ignore_mask = ~positive_mask & ~negative_mask\n",
        "\n",
        "    return matched_gt_index, positive_mask, ignore_mask\n",
        "\n",
        "# 예제 사용법\n",
        "anchor_generator = AnchorBox()\n",
        "image_size = (800, 800)  # 이미지 크기 (높이, 너비)\n",
        "anchors = anchor_generator.get_anchors(image_size)\n",
        "\n",
        "# GT box의 예시 (x, y, w, h 형태)\n",
        "gt_boxes = torch.tensor([[100, 100, 50, 50], [300, 300, 80, 80]], dtype=torch.float32)\n",
        "\n",
        "match_iou = 0.5  # positive로 매칭할 IoU 임계값\n",
        "ignore_iou = 0.4  # ignore으로 처리할 IoU 임계값\n",
        "\n",
        "matched_gt_index, positive_mask, ignore_mask = match_anchors_with_gt(anchors, gt_boxes, match_iou, ignore_iou)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"매칭된 GT box 인덱스:\", matched_gt_index)\n",
        "print(\"Positive mask:\", positive_mask)\n",
        "print(\"Ignore mask:\", ignore_mask)\n"
      ],
      "metadata": {
        "id": "zpEMh2YUsJrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTNkKJnSFKoB"
      },
      "source": [
        "### 문제 14. _compute_box_target method\n",
        "    - 각 anchor box에 대한 transform 값을 계산합니다.\n",
        "    - transform은 gt box와 같아지기 위해서 anchor box의 x,y,w,h를 얼마나 변형해야 하는가에 대한 값입니다.\n",
        "    - 입력값으로 각 anchor box와 그에 대한 target 값을 받습니다.\n",
        "    - 반환값은 각 anchor box의 transform label입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def calculate_anchor_transforms(anchors, targets):\n",
        "    \"\"\"\n",
        "    각 Anchor box에 대한 transform 값을 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        anchors (torch.Tensor): Anchor box의 (x, y, w, h) 좌표 정보 (M, 4), M은 Anchor box의 개수.\n",
        "        targets (torch.Tensor): 각 Anchor box에 대한 target 값의 (x, y, w, h) 좌표 정보 (M, 4).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 각 Anchor box의 transform label (M, 4).\n",
        "    \"\"\"\n",
        "    # Anchor box와 target의 좌표 정보를 (x1, y1, x2, y2) 형태로 변경\n",
        "    x1_anchors, y1_anchors, x2_anchors, y2_anchors = anchors[:, 0], anchors[:, 1], anchors[:, 0] + anchors[:, 2], anchors[:, 1] + anchors[:, 3]\n",
        "    x1_targets, y1_targets, x2_targets, y2_targets = targets[:, 0], targets[:, 1], targets[:, 0] + targets[:, 2], targets[:, 1] + targets[:, 3]\n",
        "\n",
        "    # 변형 값을 계산\n",
        "    transform_x = (x1_targets - x1_anchors) / anchors[:, 2]\n",
        "    transform_y = (y1_targets - y1_anchors) / anchors[:, 3]\n",
        "    transform_w = torch.log((x2_targets - x1_targets) / anchors[:, 2])\n",
        "    transform_h = torch.log((y2_targets - y1_targets) / anchors[:, 3])\n",
        "\n",
        "    transforms = torch.stack((transform_x, transform_y, transform_w, transform_h), dim=-1)\n",
        "\n",
        "    return transforms\n",
        "\n",
        "# 예제 사용법\n",
        "anchors = torch.tensor([[100, 100, 50, 50], [300, 300, 80, 80], [500, 500, 60, 60]], dtype=torch.float32)\n",
        "targets = torch.tensor([[110, 110, 45, 45], [310, 310, 85, 85], [520, 520, 58, 58]], dtype=torch.float32)\n",
        "\n",
        "anchor_transforms = calculate_anchor_transforms(anchors, targets)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"각 Anchor box의 transform label:\")\n",
        "print(anchor_transforms)\n"
      ],
      "metadata": {
        "id": "RaB2xUwosPFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqAAvVKQKFqZ"
      },
      "source": [
        "### 문제 15. _encode_sample method\n",
        "    - 각 anchor box에 대한 bbox와 class의 target 값을 계산합니다.\n",
        "    - _match_anchor_boxes method에서 계산한 최대 IOU의 index를 활용하여 해당되는 gt box의 정보를 찾아서 matching 해줍니다.\n",
        "    - matching돤 gt box의 x,y,w,h와 _compute_box_target method를 이용하여, box regression에 대한 label을 생성합니다.\n",
        "    - matching된 gt box의 class id와 positive_mask, ignore_mask를 이용하여 class에 대한 label을 생성합니다.\n",
        "    - 이 때 positive인 경우는 gt의 class id를 사용하고, negative인 경우는 -1로 ignore인 경우는 -2로 labeling합니다.\n",
        "    - 입력값으로 image shape(batch, height, width, channel), gt box의 (x,y,w,h), gt box의 class id를 받습니다.\n",
        "    - 최종적으로 box target과 class target을 하나로 concat하여 반환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class AnchorBox:\n",
        "    # AnchorBox 클래스의 나머지 부분은 이전 예제에서 사용한 코드를 그대로 사용합니다.\n",
        "\n",
        "def calculate_anchor_targets(image_shape, gt_boxes, gt_class_ids):\n",
        "    \"\"\"\n",
        "    각 Anchor box에 대한 bbox와 class의 target 값을 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        image_shape (tuple): 이미지 shape (batch, height, width, channel).\n",
        "        gt_boxes (torch.Tensor): GT box의 (x, y, w, h) 좌표 정보 (N, 4), N은 GT box의 개수.\n",
        "        gt_class_ids (torch.Tensor): GT box의 class id 정보 (N,).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 각 Anchor box에 대한 bbox와 class의 target 값 (M, 5), M은 Anchor box의 개수.\n",
        "    \"\"\"\n",
        "    anchor_generator = AnchorBox()\n",
        "    anchors = anchor_generator.get_anchors((image_shape[1], image_shape[2]))  # 이미지 height, width에 맞는 anchor 생성\n",
        "\n",
        "    # 최대 IoU에 해당하는 GT box 및 mask 가져오기\n",
        "    matched_gt_index, positive_mask, ignore_mask = anchor_generator._match_anchor_boxes(anchors, gt_boxes)\n",
        "\n",
        "    # GT box 정보 가져오기\n",
        "    matched_gt_boxes = gt_boxes[matched_gt_index]\n",
        "    matched_gt_class_ids = gt_class_ids[matched_gt_index]\n",
        "\n",
        "    # bbox regression label 계산\n",
        "    bbox_targets = anchor_generator._compute_box_target(anchors, matched_gt_boxes)\n",
        "\n",
        "    # class label 계산\n",
        "    class_targets = torch.ones_like(positive_mask) * -2  # 모든 anchor를 ignore로 초기화\n",
        "    class_targets[positive_mask] = matched_gt_class_ids  # positive anchor의 class label 설정\n",
        "\n",
        "    # bbox target과 class target을 하나로 concat\n",
        "    targets = torch.cat((bbox_targets, class_targets.unsqueeze(1)), dim=1)\n",
        "\n",
        "    return targets\n",
        "\n",
        "# 예제 사용법\n",
        "image_shape = (1, 800, 800, 3)  # 이미지 shape (batch, height, width, channel)\n",
        "gt_boxes = torch.tensor([[100, 100, 50, 50], [300, 300, 80, 80]], dtype=torch.float32)\n",
        "gt_class_ids = torch.tensor([0, 1], dtype=torch.int64)\n",
        "\n",
        "anchor_targets = calculate_anchor_targets(image_shape, gt_boxes, gt_class_ids)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"각 Anchor box에 대한 bbox와 class의 target 값:\")\n",
        "print(anchor_targets)\n"
      ],
      "metadata": {
        "id": "y9kaZvq-serj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GasBMcQ8MEbG"
      },
      "source": [
        "### 문제 16. encode_batch method\n",
        "    - tf.data(dataset)에 map으로 적용할 수 있도록 합니다.\n",
        "    - batch 단위로 data를 받아서, 각 image마다 위에서 작성한 method들을 활용하여 label을 생성하고,\n",
        "      그 label들을 다시 batch 단위로 묶어서 반환합니다.\n",
        "    - 입력값으로 batch 단위의 image data, gt box의 (x,y,w,h), gt box의 class id를 받습니다.\n",
        "    - 반환값은 batch 단위의 image data(입력값 그대로)와, 생성한 label입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class AnchorBox:\n",
        "    # AnchorBox 클래스의 나머지 부분은 이전 예제에서 사용한 코드를 그대로 사용합니다.\n",
        "\n",
        "def calculate_anchor_targets_batch(images, gt_boxes_list, gt_class_ids_list):\n",
        "    \"\"\"\n",
        "    배치 단위로 이미지와 GT box 정보를 입력으로 받고, 각 이미지에 대한 라벨을 생성하여 배치 단위로 반환하는 함수.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): 이미지 데이터 (batch, height, width, channel).\n",
        "        gt_boxes_list (list): 각 이미지에 대한 GT box 정보의 리스트. 각 GT box 정보는 torch.Tensor (N, 4) 형태.\n",
        "        gt_class_ids_list (list): 각 이미지에 대한 GT box의 class id 정보의 리스트. 각 class id 정보는 torch.Tensor (N,) 형태.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 입력 이미지 데이터 (batch, height, width, channel).\n",
        "        list: 각 이미지에 대한 라벨 정보의 리스트. 각 라벨 정보는 torch.Tensor (M, 5) 형태.\n",
        "    \"\"\"\n",
        "    batch_labels = []\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        image_shape = images[i].shape  # 이미지 shape 가져오기\n",
        "        gt_boxes = gt_boxes_list[i]  # 현재 이미지에 대한 GT box 정보\n",
        "        gt_class_ids = gt_class_ids_list[i]  # 현재 이미지에 대한 GT class id 정보\n",
        "\n",
        "        # 각 이미지에 대한 라벨 생성\n",
        "        labels = calculate_anchor_targets(image_shape, gt_boxes, gt_class_ids)\n",
        "        batch_labels.append(labels)\n",
        "\n",
        "    return images, batch_labels\n",
        "\n",
        "# 예제 사용법\n",
        "# 가정: batch 크기가 2이고, 각 이미지에 대한 GT box 정보 및 GT class 정보를 리스트로 제공\n",
        "batch_images = [torch.randn((800, 800, 3)), torch.randn((800, 800, 3))]\n",
        "batch_gt_boxes = [torch.tensor([[100, 100, 50, 50], [300, 300, 80, 80]], dtype=torch.float32),\n",
        "                  torch.tensor([[200, 200, 60, 60], [400, 400, 70, 70]], dtype=torch.float32)]\n",
        "batch_gt_class_ids = [torch.tensor([0, 1], dtype=torch.int64), torch.tensor([1, 2], dtype=torch.int64)]\n",
        "\n",
        "batch_images, batch_labels = calculate_anchor_targets_batch(batch_images, batch_gt_boxes, batch_gt_class_ids)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"배치 이미지 데이터 shape:\", batch_images[0].shape, batch_images[1].shape)\n",
        "print(\"배치 라벨 정보:\", batch_labels[0], batch_labels[1])\n"
      ],
      "metadata": {
        "id": "AeLIfnZJswDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iwYSOW7NMnj"
      },
      "source": [
        "class LabelEncoder:\n",
        "    def __init__(self):\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4):\n",
        "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "\n",
        "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
        "        box_target = tf.concat(\n",
        "            [\n",
        "               ##### CODE HERE #####\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        box_target = box_target / self._box_variance\n",
        "        return box_target\n",
        "\n",
        "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        label = tf.concat([box_target, cls_target], axis=-1)\n",
        "        return label\n",
        "\n",
        "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "        images_shape = tf.shape(batch_images)\n",
        "        batch_size = images_shape[0]\n",
        "\n",
        "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "        for i in range(batch_size):\n",
        "\n",
        "            ##### CODE HERE #####\n",
        "\n",
        "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "        return batch_images, labels.stack()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1IoMQFRSmkH"
      },
      "source": [
        "## Step 5. Dataset 만들기\n",
        "앞에서 작성한 함수와 class를 활용하여 model에 data를 공급하기 위한 dataset을 만들어봅시다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aId2ac5IZLA2"
      },
      "source": [
        "### 문제 17. Train/Validation dataset 만들기\n",
        "    - 위에서 생성한 train_dataset, validation_dataset과 그동안 작성한 함수와 class들을 활용하여,\n",
        "      train/validation dataset을 만듭니다.\n",
        "    - 다음과 같은 순서로 적용합니다.\n",
        "      1. preprocess_data 함수 적용\n",
        "      2. dataset shuffle\n",
        "      3. padded_batch를 이용하여 batch로 묶음\n",
        "        - 이 때, padding 값은 image, box 좌표, class id에 대하여 각각 0.0, 1e-8, -1로 합니다.\n",
        "      4. LabelEncoder의 encode_batch 함수 적용\n",
        "      5. prefetch 적용\n",
        "    - validation_dataset에는 shuffle만 빼고 동일하게 적용합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# 주어진 데이터셋 클래스로부터 train_dataset과 validation_dataset 생성\n",
        "train_dataset = YourCustomTrainDataset()  # YourCustomTrainDataset에 실제 데이터셋 클래스 이름을 적용해야 합니다.\n",
        "validation_dataset = YourCustomValidationDataset()  # YourCustomValidationDataset에 실제 데이터셋 클래스 이름을 적용해야 합니다.\n",
        "\n",
        "# 데이터 전처리를 위한 함수 정의\n",
        "def preprocess_data(image, gt_boxes, gt_class_ids):\n",
        "    # 이미지 전처리 (예시)\n",
        "    image = transforms.ToTensor()(image)  # 이미지를 텐서로 변환\n",
        "    image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)  # 이미지를 정규화\n",
        "\n",
        "    return image, gt_boxes, gt_class_ids\n",
        "\n",
        "# DataLoader의 batch_size 설정\n",
        "batch_size = 32\n",
        "\n",
        "# train_dataset에 대한 DataLoader 설정\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: list(zip(*x)))\n",
        "\n",
        "# validation_dataset에 대한 DataLoader 설정\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: list(zip(*x)))\n",
        "\n",
        "# LabelEncoder 인스턴스 생성\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# train_loader와 validation_loader에 대해 전처리 및 라벨 인코딩 적용\n",
        "for data in train_loader:\n",
        "    images, gt_boxes, gt_class_ids = data\n",
        "    images, gt_boxes, gt_class_ids = preprocess_data(images, gt_boxes, gt_class_ids)\n",
        "    labels = calculate_anchor_targets_batch([images], [gt_boxes], [gt_class_ids])[1]\n",
        "    label_encoder.encode_batch(labels)\n",
        "\n",
        "for data in validation_loader:\n",
        "    images, gt_boxes, gt_class_ids = data\n",
        "    images, gt_boxes, gt_class_ids = preprocess_data(images, gt_boxes, gt_class_ids)\n",
        "    labels = calculate_anchor_targets_batch([images], [gt_boxes], [gt_class_ids])[1]\n",
        "    label_encoder.encode_batch(labels)\n",
        "\n",
        "# DataLoader에 prefetch 설정 (마지막 인자는 CPU 스레드 수로 조절 가능)\n",
        "train_loader = train_loader.prefetch(2)\n",
        "validation_loader = validation_loader.prefetch(2)\n"
      ],
      "metadata": {
        "id": "GfBLM3eYs6tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0SZL_vUNMq_"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "autotune = tf.data.AUTOTUNE\n",
        "\n",
        "##### CODE HERE #####\n",
        "\n",
        "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "train_dataset = train_dataset.prefetch(autotune)\n",
        "\n",
        "\n",
        "##### CODE HERE #####\n",
        "\n",
        "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "val_dataset = val_dataset.prefetch(autotune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eka4tTWbcgOO"
      },
      "source": [
        "## Step 6. RetinaNet Model 만들기\n",
        "Data가 준비되었으므로, RetinaNet model을 만들어보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niV-FXmvcrrR"
      },
      "source": [
        "### 문제 18. get_backbone 함수 만들기\n",
        "    - backbone으로 ResNet50을 사용합니다.\n",
        "    - get_backbone 함수는 ResNet50의 C3, C4, C5에 해당하는 feature map을 output으로 하는 model을 반환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_backbone():\n",
        "    # 미리 학습된 ResNet-50 모델을 불러옴\n",
        "    resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "    # ResNet-50의 마지막 fully connected 레이어를 제거하여 feature extractor로 활용\n",
        "    # 이 때, C3, C4, C5에 해당하는 feature map을 반환하도록 설정\n",
        "    backbone = nn.Sequential(\n",
        "        *list(resnet50.children())[:-2]\n",
        "    )\n",
        "\n",
        "    return backbone\n",
        "\n",
        "# ResNet-50 백본 모델 생성\n",
        "backbone_model = get_backbone()\n",
        "\n",
        "# 예제로 백본 모델의 아웃풋 shape 확인\n",
        "input_tensor = torch.randn(1, 3, 224, 224)  # 입력 이미지 shape (batch, channel, height, width)\n",
        "output_feature_maps = backbone_model(input_tensor)\n",
        "\n",
        "# 각 feature map의 shape 출력\n",
        "for i, feature_map in enumerate(output_feature_maps):\n",
        "    print(f\"Feature Map C{i+3} shape: {feature_map.shape}\")\n"
      ],
      "metadata": {
        "id": "dQRvminStBt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBzL5ebweUZh"
      },
      "source": [
        "먼저 ResNet50을 가져와서 C3, C4, C5에 해당하는 layer의 이름을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_RqC-0Mcghi"
      },
      "source": [
        "backbone = keras.applications.ResNet50(include_top=False, input_shape=[224, 224, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw0NuGgPcgke"
      },
      "source": [
        "backbone.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk4mpjqXe3Ba"
      },
      "source": [
        "확인된 layer 이름을 활용하여 get_backbone 함수를 작성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjQE-1ZJcgnQ"
      },
      "source": [
        "def get_backbone():\n",
        "    backbone = keras.applications.ResNet50(\n",
        "        include_top=False, input_shape=[None, None, 3]\n",
        "    )\n",
        "\n",
        "    ##### CODE HERE #####\n",
        "\n",
        "    return keras.Model(\n",
        "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gouQxDVCfHFL"
      },
      "source": [
        "### 문제 19. Feature Pyramid Network 만들기\n",
        "keras custom layer를 활용하여 FPN을 만들어봅시다.\n",
        "\n",
        "    - input으로 batch 단위의 image를 받고 output으로 P3-P7을 반환합니다.\n",
        "    - RetinaNet과 FPN 논문의 Feature Pyramid Network 부분을 참고하여 작성합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "class FPNBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FPNBackbone, self).__init__()\n",
        "        # ResNet-50 백본을 불러옴\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        # ResNet-50의 마지막 fully connected 레이어를 제거하여 feature extractor로 활용\n",
        "        self.backbone = nn.Sequential(\n",
        "            *list(self.resnet.children())[:-2]\n",
        "        )\n",
        "\n",
        "        # Lateral Convolution Layers\n",
        "        self.lateral_c3 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.lateral_c4 = nn.Conv2d(1024, 256, kernel_size=1)\n",
        "        self.lateral_c5 = nn.Conv2d(2048, 256, kernel_size=1)\n",
        "\n",
        "        # P6 and P7 Layers\n",
        "        self.p6 = nn.Conv2d(2048, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.p7 = nn.ReLU()(self.p6)\n",
        "\n",
        "        # P3, P4, P5 Layers\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, x):\n",
        "        c2, c3, c4, c5 = self.backbone(x)\n",
        "\n",
        "        # Lateral Connections\n",
        "        p5 = self.lateral_c5(c5)\n",
        "        p4 = self.lateral_c4(c4) + self.upsample(p5)\n",
        "        p3 = self.lateral_c3(c3) + self.upsample(p4)\n",
        "\n",
        "        # P6 and P7\n",
        "        p6 = self.p6(c5)\n",
        "        p7 = self.p7(p6)\n",
        "\n",
        "        return p3, p4, p5, p6, p7\n",
        "\n",
        "# 백본 모델 생성\n",
        "fpn_backbone = FPNBackbone()\n",
        "\n",
        "# 예제로 백본 모델에 이미지를 전달하여 P3-P7 feature map을 확인\n",
        "input_tensor = torch.randn(1, 3, 224, 224)  # 입력 이미지 shape (batch, channel, height, width)\n",
        "output_feature_maps = fpn_backbone(input_tensor)\n",
        "\n",
        "# 각 feature map의 shape 출력\n",
        "for i, feature_map in enumerate(output_feature_maps):\n",
        "    print(f\"Feature Map P{i+3} shape: {feature_map.shape}\")\n"
      ],
      "metadata": {
        "id": "zL4WHJaUtJpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFAqabf8HkbN"
      },
      "source": [
        "class FeaturePyramid(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
        "        self.backbone = get_backbone()\n",
        "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        return p3_output, p4_output, p5_output, p6_output, p7_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV7IFjwglCYh"
      },
      "source": [
        "### 문제 20. Class/Box subnet 만들기\n",
        "classification과 box regression을 위한 subnet(head)를 만들어봅시다.\n",
        "\n",
        "    - classification과 box regression의 subnet 구조가 동일하므로 공통으로 사용할 수 있는 함수 형태로 작성합니다.\n",
        "    - 이 함수는 입력값으로 마지막 layer의 convolution filter 수와 bias initializaer를 받습니다.\n",
        "    - channel이 256인 feature map을 입력으로 classification 혹은 box regression의 결과를 뽑아주는 모델을 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def classification_subnet(num_filters, bias_initializer=None):\n",
        "    \"\"\"\n",
        "    Classification 또는 Box Regression 서브넷을 생성하는 함수.\n",
        "\n",
        "    Args:\n",
        "        num_filters (int): 마지막 레이어의 필터 수.\n",
        "        bias_initializer (callable, optional): 바이어스 초기화 함수. 기본값은 None.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Classification 또는 Box Regression 서브넷 모델.\n",
        "    \"\"\"\n",
        "    subnet = nn.Sequential(\n",
        "        nn.Conv2d(256, num_filters, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "    # 마지막 레이어의 바이어스 초기화 설정\n",
        "    if bias_initializer is not None:\n",
        "        subnet[-1].bias.data.fill_(bias_initializer)\n",
        "\n",
        "    return subnet\n",
        "\n",
        "# Classification 서브넷 모델 생성 예제\n",
        "classification_subnet_model = classification_subnet(80, bias_initializer=0.0)\n",
        "print(classification_subnet_model)\n",
        "\n",
        "# Box Regression 서브넷 모델 생성 예제\n",
        "box_regression_subnet_model = classification_subnet(4, bias_initializer=0.0)\n",
        "print(box_regression_subnet_model)\n"
      ],
      "metadata": {
        "id": "k8Cw4YIstTI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t66npUEOb4d"
      },
      "source": [
        "def build_head(output_filters, bias_init):\n",
        "    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n",
        "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
        "\n",
        "    ##### CODE HERE #####\n",
        "\n",
        "    head.add(\n",
        "        keras.layers.Conv2D(\n",
        "            output_filters,\n",
        "            3,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            bias_initializer=bias_init,\n",
        "        )\n",
        "    )\n",
        "    return head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3lbYVRem5zY"
      },
      "source": [
        "### 문제 21. RetinaNet model 만들기\n",
        "위에서 작성한 layer 및 함수를 이용하여 RetinaNet model을 만들어봅시다.\n",
        "\n",
        "    - keras subclassing model을 이용하여 작성합니다.\n",
        "    - 입력으로 image를 받고, 출력으로 모든 anchor box의 regression과 classification 예측값을 모아서 반환합니다.\n",
        "    - classification subnet에는 RetinaNet 논문에 나온 bias initialzation을 적용합니다.\n",
        "    - output shape은 (batch size, 전체 anchor box의 갯수, num_classes+4[84]) 입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def retina_subnet(input_channels, num_classes, num_anchors):\n",
        "    \"\"\"\n",
        "    RetinaNet의 Classification 또는 Box Regression 서브넷을 생성하는 함수.\n",
        "\n",
        "    Args:\n",
        "        input_channels (int): 입력 feature map의 채널 수 (256).\n",
        "        num_classes (int): 객체 클래스의 수.\n",
        "        num_anchors (int): 각 위치에서 예측할 anchor box의 수.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: RetinaNet의 Classification 또는 Box Regression 서브넷 모델.\n",
        "    \"\"\"\n",
        "    # Bias 초기화 값을 계산\n",
        "    prior_prob = 0.01\n",
        "    bias_initializer = -torch.log(torch.tensor((1 - prior_prob) / prior_prob))\n",
        "\n",
        "    subnet = nn.Sequential(\n",
        "        nn.Conv2d(input_channels, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, num_classes * num_anchors, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 4 * num_anchors, kernel_size=3, stride=1, padding=1),\n",
        "    )\n",
        "\n",
        "    # 마지막 레이어의 바이어스 초기화 설정\n",
        "    nn.init.constant_(subnet[-1].bias.data, bias_initializer)\n",
        "\n",
        "    return subnet\n",
        "\n",
        "# 예시로 모델 생성\n",
        "input_channels = 256  # 입력 feature map 채널 수\n",
        "num_classes = 20  # 객체 클래스의 수\n",
        "num_anchors = 9  # 각 위치에서 예측할 anchor box의 수\n",
        "\n",
        "retina_subnet_model = retina_subnet(input_channels, num_classes, num_anchors)\n",
        "print(retina_subnet_model)\n",
        "\n",
        "# 입력 이미지 생성 (예시)\n",
        "batch_size = 2\n",
        "input_image = torch.randn(batch_size, input_channels, 224, 224)  # (batch, channels, height, width)\n",
        "\n",
        "# 모델에 입력 이미지 전달하여 예측 수행\n",
        "predictions = retina_subnet_model(input_image)\n",
        "\n",
        "# 예측 결과의 shape 출력\n",
        "print(\"예측 결과 shape:\", predictions.shape)\n"
      ],
      "metadata": {
        "id": "7CyJTYx-tZlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Qc8EQHGnLh"
      },
      "source": [
        "class RetinaNet(keras.Model):\n",
        "    def __init__(self, num_classes, **kwargs):\n",
        "        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n",
        "        self.fpn = FeaturePyramid()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "        self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "    def call(self, image, training=False):\n",
        "        features = self.fpn(image, training=training)\n",
        "        N = tf.shape(image)[0]\n",
        "        cls_outputs = []\n",
        "        box_outputs = []\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
        "        box_outputs = tf.concat(box_outputs, axis=1)\n",
        "        return tf.concat([box_outputs, cls_outputs], axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKMgFVJbl_n0"
      },
      "source": [
        "## Step 7. Model 학습하기\n",
        "Loss function을 정의하고 RetinaNet model을 학습해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtBCGTbKmM6f"
      },
      "source": [
        "### 문제 22. Smooth L1 loss\n",
        "    - box regression을 위한 smooth l1 loss를 만들어봅시다.\n",
        "    - tf.losses.Loss class의 subclass로 custom loss를 만듭니다.\n",
        "    - l1, l2 loss가 변하는 지점의 값은 delta 값으로 __init__ method에서 입력받습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SmoothL1Loss(nn.Module):\n",
        "    def __init__(self, l1_threshold, l2_threshold):\n",
        "        \"\"\"\n",
        "        Smooth L1 Loss를 초기화합니다.\n",
        "\n",
        "        Args:\n",
        "            l1_threshold (float): L1 Loss와 L2 Loss가 변하는 지점의 임계값.\n",
        "            l2_threshold (float): L2 Loss와 L1 Loss가 변하는 지점의 임계값.\n",
        "        \"\"\"\n",
        "        super(SmoothL1Loss, self).__init__()\n",
        "        self.l1_threshold = l1_threshold\n",
        "        self.l2_threshold = l2_threshold\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        \"\"\"\n",
        "        Smooth L1 Loss를 계산합니다.\n",
        "\n",
        "        Args:\n",
        "            prediction (torch.Tensor): 예측값.\n",
        "            target (torch.Tensor): 실제값.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Smooth L1 Loss.\n",
        "        \"\"\"\n",
        "        diff = torch.abs(prediction - target)\n",
        "        loss = torch.where(diff < self.l1_threshold, 0.5 * diff**2, diff - 0.5 * self.l1_threshold**2)\n",
        "        loss = torch.where(diff < self.l2_threshold, loss, 0.5 * self.l2_threshold**2 + self.l2_threshold * (diff - self.l2_threshold))\n",
        "        return loss\n",
        "\n",
        "# 예제로 Loss 객체 생성\n",
        "l1_threshold = 1.0\n",
        "l2_threshold = 5.0\n",
        "smooth_l1_loss = SmoothL1Loss(l1_threshold, l2_threshold)\n",
        "\n",
        "# 예측값과 실제값 생성 (예시)\n",
        "prediction = torch.tensor([2.0, 4.0, 6.0])\n",
        "target = torch.tensor([1.0, 3.0, 5.0])\n",
        "\n",
        "# Smooth L1 Loss 계산\n",
        "loss = smooth_l1_loss(prediction, target)\n",
        "print(\"Smooth L1 Loss:\", loss.item())\n"
      ],
      "metadata": {
        "id": "Ox7HtXZTtjE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRL4nXbUpXfJ"
      },
      "source": [
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super(RetinaNetBoxLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        return tf.reduce_sum(loss, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b8qWFAwnnzC"
      },
      "source": [
        "### 문제 23. Focal loss\n",
        "    - classification을 위한 focal loss를 만들어봅시다.\n",
        "    - smooth l1 loss와 마찬가지로 subclassing을 이용하고, 논문의 수식을 참고하여 구현합니다.\n",
        "    - alpha, gamma 값은 __init__ method에서 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha, gamma):\n",
        "        \"\"\"\n",
        "        Focal Loss를 초기화합니다.\n",
        "\n",
        "        Args:\n",
        "            alpha (float): 클래스별 가중치 (0 이상의 값).\n",
        "            gamma (float): Focal Loss의 감마 값 (gamma >= 0).\n",
        "        \"\"\"\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        \"\"\"\n",
        "        Focal Loss를 계산합니다.\n",
        "\n",
        "        Args:\n",
        "            prediction (torch.Tensor): 예측값.\n",
        "            target (torch.Tensor): 실제값.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Focal Loss.\n",
        "        \"\"\"\n",
        "        # Cross-Entropy Loss 계산\n",
        "        ce_loss = F.cross_entropy(prediction, target, reduction='none')\n",
        "\n",
        "        # 클래스별 가중치 적용\n",
        "        alpha_factor = torch.exp(-self.alpha * (1 - F.softmax(prediction, dim=1)))\n",
        "        focal_loss = alpha_factor * ce_loss**self.gamma\n",
        "\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# 예제로 Loss 객체 생성\n",
        "alpha = 0.25\n",
        "gamma = 2.0\n",
        "focal_loss = FocalLoss(alpha, gamma)\n",
        "\n",
        "# 예측값과 실제값 생성 (예시)\n",
        "prediction = torch.randn(3, 5)  # 3개의 예제, 5개의 클래스\n",
        "target = torch.tensor([1, 3, 2])  # 각 예제의 정답 클래스\n",
        "\n",
        "# Focal Loss 계산\n",
        "loss = focal_loss(prediction, target)\n",
        "print(\"Focal Loss:\", loss.item())\n"
      ],
      "metadata": {
        "id": "ASjkZCtbtraj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPLQtAuanbL0"
      },
      "source": [
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Focal loss\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super(RetinaNetClassificationLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        return tf.reduce_sum(loss, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT2PxGIIoVAw"
      },
      "source": [
        "### 문제 24. RetinaNet loss\n",
        "    - smooth l1 loss와 focal loss를 합쳐서 RetinaNet loss를 만들어봅시다.\n",
        "    - classification loss는 ignore의 경우를 제외합니다.\n",
        "    - box regression loss는 positive에 대해서만 계산합니다.\n",
        "    - 각 loss는 positive sample의 갯수로 normalize합니다.\n",
        "    - 최종 loss는 classification loss와 regression loss를 더해서 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RetinaNetLoss(nn.Module):\n",
        "    def __init__(self, alpha, gamma, l1_threshold, l2_threshold):\n",
        "        \"\"\"\n",
        "        RetinaNet의 Loss를 초기화합니다.\n",
        "\n",
        "        Args:\n",
        "            alpha (float): Focal Loss의 클래스별 가중치.\n",
        "            gamma (float): Focal Loss의 감마 값.\n",
        "            l1_threshold (float): Smooth L1 Loss의 L1 Loss와 L2 Loss가 변하는 지점의 임계값.\n",
        "            l2_threshold (float): Smooth L1 Loss의 L2 Loss와 L1 Loss가 변하는 지점의 임계값.\n",
        "        \"\"\"\n",
        "        super(RetinaNetLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.l1_threshold = l1_threshold\n",
        "        self.l2_threshold = l2_threshold\n",
        "        self.smooth_l1_loss = SmoothL1Loss(l1_threshold, l2_threshold)\n",
        "        self.focal_loss = FocalLoss(alpha, gamma)\n",
        "\n",
        "    def forward(self, classification_predictions, regression_predictions,\n",
        "                classification_targets, regression_targets,\n",
        "                positive_masks, ignore_masks):\n",
        "        \"\"\"\n",
        "        RetinaNet의 Loss를 계산합니다.\n",
        "\n",
        "        Args:\n",
        "            classification_predictions (torch.Tensor): 객체 클래스 예측값.\n",
        "            regression_predictions (torch.Tensor): 바운딩 박스 예측값.\n",
        "            classification_targets (torch.Tensor): 객체 클래스 실제값.\n",
        "            regression_targets (torch.Tensor): 바운딩 박스 실제값.\n",
        "            positive_masks (torch.Tensor): Positive 샘플 마스크.\n",
        "            ignore_masks (torch.Tensor): Ignore 샘플 마스크.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: RetinaNet의 Loss.\n",
        "        \"\"\"\n",
        "        # Classification Loss 계산 (Ignore 샘플 제외)\n",
        "        classification_loss = self.focal_loss(classification_predictions[~ignore_masks],\n",
        "                                               classification_targets[~ignore_masks])\n",
        "\n",
        "        # Box Regression Loss 계산 (Positive 샘플만 계산)\n",
        "        positive_regression_targets = regression_targets[positive_masks]\n",
        "        positive_regression_predictions = regression_predictions[positive_masks]\n",
        "        regression_loss = self.smooth_l1_loss(positive_regression_predictions, positive_regression_targets)\n",
        "\n",
        "        # Positive 샘플 수로 정규화\n",
        "        num_positive_samples = positive_masks.sum()\n",
        "        classification_loss /= num_positive_samples\n",
        "        regression_loss /= num_positive_samples\n",
        "\n",
        "        # 최종 Loss는 Classification Loss와 Regression Loss의 합\n",
        "        total_loss = classification_loss + regression_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "# 예제로 Loss 객체 생성\n",
        "alpha = 0.25\n",
        "gamma = 2.0\n",
        "l1_threshold = 1.0\n",
        "l2_threshold = 5.0\n",
        "retina_loss = RetinaNetLoss(alpha, gamma, l1_threshold, l2_threshold)\n",
        "\n",
        "# 예측값과 실제값, 마스크 생성 (예시)\n",
        "classification_predictions = torch.randn(10, 20)  # (batch, num_anchors, num_classes)\n",
        "regression_predictions = torch.randn(10, 9, 4)  # (batch, num_anchors, 4)\n",
        "classification_targets = torch.randint(0, 20, (10, 9))  # (batch, num_anchors)\n",
        "regression_targets = torch.randn(10, 9, 4)  # (batch, num_anchors, 4)\n",
        "positive_masks = torch.randint(0, 2, (10, 9), dtype=torch.bool)  # (batch, num_anchors)\n",
        "ignore_masks = torch.randint(0, 2, (10, 9), dtype=torch.bool)  # (batch, num_anchors)\n",
        "\n",
        "# RetinaNet Loss 계산\n",
        "loss = retina_loss(classification_predictions, regression_predictions,\n",
        "                   classification_targets, regression_targets,\n",
        "                   positive_masks, ignore_masks)\n",
        "print(\"RetinaNet Loss:\", loss.item())\n"
      ],
      "metadata": {
        "id": "scw0xm6XtzUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlcx0P0woUHM"
      },
      "source": [
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
        "        self._cls_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        loss = cls_loss + box_loss\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_jl1v5L5vAp"
      },
      "source": [
        "### 문제 25. Model compile\n",
        "    - RetinaNet model을 만들고, RetenaNet loss를 이용하여 model을 compile 해봅시다.\n",
        "    - optimizer는 momentum SGD를 사용하고, momentum 값은 0.9를 사용합니다.\n",
        "    - learning rate schedule은 위에서 hyperparameter 설정부분에서 만들어 둔 것을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# RetinaNet 모델 정의\n",
        "class RetinaNet(nn.Module):\n",
        "    def __init__(self, num_classes, num_anchors):\n",
        "        super(RetinaNet, self).__init__()\n",
        "        # 모델 구성 요소 정의 (Backbone, FPN, Subnets 등)\n",
        "        # ...\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        # Forward Pass 구현\n",
        "        # ...\n",
        "\n",
        "# 모델 초기화\n",
        "num_classes = 20  # 예시: 객체 클래스 수\n",
        "num_anchors = 9  # 예시: 각 위치에서 예측할 anchor box 수\n",
        "retina_model = RetinaNet(num_classes, num_anchors)\n",
        "\n",
        "# RetinaNet Loss 생성\n",
        "alpha = 0.25\n",
        "gamma = 2.0\n",
        "l1_threshold = 1.0\n",
        "l2_threshold = 5.0\n",
        "retina_loss = RetinaNetLoss(alpha, gamma, l1_threshold, l2_threshold)\n",
        "\n",
        "# Optimizer 설정 (SGD with Momentum)\n",
        "optimizer = optim.SGD(retina_model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Learning Rate Schedule 설정 (예시: 위에서 정의한 학습률 스케줄러)\n",
        "scheduler = LearningRateScheduler(max_lr, warmup_steps, decay_steps)\n",
        "\n",
        "# 학습률 스케줄러를 Optimizer에 등록\n",
        "optimizer = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
        "\n",
        "# 모델 컴파일\n",
        "retina_model.compile(loss=retina_loss, optimizer=optimizer)\n",
        "\n",
        "# 모델 정보 출력\n",
        "print(retina_model)\n"
      ],
      "metadata": {
        "id": "nh1M0f13uByH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKv6IPOC5dLa"
      },
      "source": [
        "##### CODE HERE #####\n",
        "\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHoULfsQ6d43"
      },
      "source": [
        "### Checkpoint callback\n",
        "    - 다음과 같이 model의 weight를 저장하기 위한 checkpoint callback을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CheckpointCallback:\n",
        "    def __init__(self, model, save_path, save_best_only=True):\n",
        "        \"\"\"\n",
        "        모델의 가중치를 저장하기 위한 Checkpoint Callback을 초기화합니다.\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): 저장할 모델.\n",
        "            save_path (str): 가중치를 저장할 경로와 파일 이름.\n",
        "            save_best_only (bool): 최상의 성능을 가진 모델만 저장할지 여부.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.save_path = save_path\n",
        "        self.save_best_only = save_best_only\n",
        "        self.best_loss = float('inf') if save_best_only else None\n",
        "\n",
        "    def __call__(self, current_loss):\n",
        "        \"\"\"\n",
        "        Callback을 호출하여 모델의 가중치를 저장합니다.\n",
        "\n",
        "        Args:\n",
        "            current_loss (float): 현재 Loss 값.\n",
        "        \"\"\"\n",
        "        if self.save_best_only and current_loss >= self.best_loss:\n",
        "            return  # 현재 Loss가 최상이 아니면 저장하지 않음\n",
        "\n",
        "        # 모델의 가중치 저장\n",
        "        torch.save(self.model.state_dict(), self.save_path)\n",
        "        print(f\"Model weights saved to {self.save_path}\")\n",
        "        if self.save_best_only:\n",
        "            self.best_loss = current_loss\n",
        "\n",
        "# 예제로 Callback 객체 생성 및 사용\n",
        "model = RetinaNet(num_classes, num_anchors)  # 예시: RetinaNet 모델\n",
        "save_path = 'model_weights.pth'  # 저장 경로 및 파일 이름 지정\n",
        "checkpoint_callback = CheckpointCallback(model, save_path)\n",
        "\n",
        "# 모델 학습 루프 내에서 Callback 호출 (예시)\n",
        "current_loss = 0.5  # 현재 Loss 값\n",
        "checkpoint_callback(current_loss)  # 모델 가중치 저장\n"
      ],
      "metadata": {
        "id": "_ZTsaSiIuGBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH5ia2wp6XI4"
      },
      "source": [
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=False,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljhAPyHi6w_L"
      },
      "source": [
        "### Model 학습하기\n",
        "    - model.fit으로 model을 학습합니다.\n",
        "    - MS-COCO 전체 data를 이용하여 학습하는 경우에는 아래 주석 처리된 부분의 주석을 지우고,\n",
        "      epochs = 1 부분을 삭제,\n",
        "      train_dataset.take(100), val_dataset.take(50)의 .take 부분을 삭제하면 학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.models.detection import RetinaNet\n",
        "from torchvision.models.detection.retinanet import retinanet_resnet50_fpn\n",
        "\n",
        "# 학습 설정\n",
        "learning_rate = 0.001\n",
        "batch_size = 2\n",
        "epochs = 1  # 학습 에폭 수\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# MS-COCO 데이터셋 로드 (경로는 본인 환경에 맞게 수정)\n",
        "train_dataset = CocoDetection(root='path_to_train_dataset', annFile='annotations_train.json', transform=transform)\n",
        "val_dataset = CocoDetection(root='path_to_val_dataset', annFile='annotations_val.json', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# RetinaNet 모델 로드\n",
        "model = retinanet_resnet50_fpn(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer 및 Loss 함수 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "criterion = torch.nn.SmoothL1Loss()  # Smooth L1 Loss를 사용하거나 자신의 Loss 함수를 지정하세요.\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for images, targets in train_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 검증\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_losses = []\n",
        "        for images, targets in val_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            val_losses.append(losses.item())\n",
        "\n",
        "        mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {mean_val_loss:.4f}\")\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), 'retinanet_model.pth')\n",
        "ㅑ"
      ],
      "metadata": {
        "id": "5L8RlpvPuNHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AYgR9ux6pHP"
      },
      "source": [
        "# train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n",
        "# val_steps_per_epoch = \\\n",
        "#     dataset_info.splits[\"validation\"].num_examples // batch_size\n",
        "\n",
        "# train_steps = 4 * 100000\n",
        "# epochs = train_steps // train_steps_per_epoch\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "model.fit(\n",
        "    train_dataset.take(100),\n",
        "    validation_data=val_dataset.take(50),\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at9VOq-jOUYv"
      },
      "source": [
        "## Step 8. 학습된 Model로 결과 확인하기\n",
        "학습된 model의 weight를 불러와서 validation set에 대하여 detection 결과를 확인해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1L6hbQOPQLI"
      },
      "source": [
        "### 학습된 Weights Loading\n",
        "    - 미리 학습해둔 weights를 아래와 같이 다운받고, model에 load 합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models.detection\n",
        "\n",
        "# 학습된 RetinaNet 모델의 가중치를 다운로드\n",
        "model_url = \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\"\n",
        "model_weights_path = \"retinanet_resnet50_fpn_coco.pth\"\n",
        "torch.hub.download_url_to_file(model_url, model_weights_path)\n",
        "\n",
        "# 학습된 가중치를 사용하여 RetinaNet 모델 초기화\n",
        "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False)  # 미리 학습된 가중치 사용하지 않음\n",
        "model.load_state_dict(torch.load(model_weights_path))  # 학습된 가중치 로드\n",
        "model.eval()  # 모델을 추론 모드로 설정 (학습 모드가 아닌 추론 모드)\n"
      ],
      "metadata": {
        "id": "NT9jh4nUuhSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX6iEshc-ngV"
      },
      "source": [
        "ckpt_url = \"https://drive.google.com/uc?id=19snoNsuyeLPxkj9Is1cMmY-JviTydZim\"\n",
        "\n",
        "gdown.download(ckpt_url, 'ckpt.zip', quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(\"ckpt.zip\", \"r\") as z_fp:\n",
        "    z_fp.extractall(\"./ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXQ6VPLtMx1y"
      },
      "source": [
        "# Change this to `model_dir` when not using the downloaded weights\n",
        "weights_dir = \"ckpt\"\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
        "model.load_weights(latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6hddicGPagj"
      },
      "source": [
        "### Model 예측 결과를 Decoding하기\n",
        "RetinaNet은 anchor box 내의 물체에 대한 class별 확률과 anchor box를 얼마나 변형시킬지의 대한 값을 예측합니다.\n",
        "\n",
        "이를 이용하여 bounding box를 transform하고, class에 대한 예측을 계산합니다.\n",
        "\n",
        "이를 위하여 DecodePredictions 라는 custom layer를 생성하고, model의 prediction에 이 layer를 통과시켜서 최종 예측값을 얻는 형태로 구현합니다.\n",
        "\n",
        "DecodePredictions의 method는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak7-I0KEXJgV"
      },
      "source": [
        "### \\_\\_init\\_\\_ method\n",
        "     - anchor box를 생성하고, nms를 위한 thresould 값들, 그리고 maximum detection의 수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.ops as ops\n",
        "\n",
        "class DecodePredictions(nn.Module):\n",
        "    def __init__(self, num_classes, anchors, score_threshold=0.05, nms_iou_threshold=0.5, max_detections=100):\n",
        "        \"\"\"\n",
        "        DecodePredictions 레이어의 생성자입니다.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): 객체 클래스 수.\n",
        "            anchors (list of torch.Tensor): 모델에서 사용하는 anchor box의 정보 (x, y, w, h).\n",
        "            score_threshold (float): 예측된 객체 확률 중 임계값을 지정합니다.\n",
        "            nms_iou_threshold (float): Non-Maximum Suppression (NMS)를 위한 IOU 임계값.\n",
        "            max_detections (int): 최대 검출 수.\n",
        "        \"\"\"\n",
        "        super(DecodePredictions, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.anchors = anchors\n",
        "        self.score_threshold = score_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "    def forward(self, predictions):\n",
        "        # 여기서 predictions는 모델의 출력으로 주어진 것을 가정합니다.\n",
        "        # 예측된 클래스 확률, box 변환 정보, anchor box 등을 포함합니다.\n",
        "\n",
        "        # 예측된 박스 변환 정보 및 클래스 확률 얻기\n",
        "        class_probs, box_deltas = predictions\n",
        "        batch_size, num_anchors, _, _ = box_deltas.shape\n",
        "\n",
        "        # 최대 검출 수에 맞게 상위 점수를 가진 예측 선택\n",
        "        scores = class_probs.sigmoid().max(dim=2)[0]  # 클래스 확률 중 최대값\n",
        "        scores = scores.view(batch_size, num_anchors)\n",
        "\n",
        "        top_scores, top_class_indices = scores.topk(self.max_detections, dim=1)\n",
        "\n",
        "        # NMS를 적용하여 중복 검출 제거\n",
        "        detections = []\n",
        "        for batch_idx in range(batch_size):\n",
        "            boxes = self.decode_boxes(box_deltas[batch_idx], self.anchors)\n",
        "            boxes = boxes[top_class_indices[batch_idx]]\n",
        "            scores = top_scores[batch_idx]\n",
        "\n",
        "            indices = ops.nms(boxes, scores, self.nms_iou_threshold)\n",
        "            boxes = boxes[indices]\n",
        "            scores = scores[indices]\n",
        "\n",
        "            # 점수가 임계값 이상인 예측만 유지\n",
        "            mask = scores > self.score_threshold\n",
        "            boxes = boxes[mask]\n",
        "            scores = scores[mask]\n",
        "\n",
        "            # 최종 검출 결과에 추가\n",
        "            detections.append(torch.cat((boxes, scores.unsqueeze(1)), dim=1))\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def decode_boxes(self, deltas, anchors):\n",
        "        widths = anchors[:, 2] - anchors[:, 0]\n",
        "        heights = anchors[:, 3] - anchors[:, 1]\n",
        "        x_center = anchors[:, 0] + 0.5 * widths\n",
        "        y_center = anchors[:, 1] + 0.5 * heights\n",
        "\n",
        "        dx, dy, dw, dh = deltas.split(1, dim=1)\n",
        "        dx, dy, dw, dh = dx.squeeze(1), dy.squeeze(1), dw.squeeze(1), dh.squeeze(1)\n",
        "\n",
        "        width = torch.exp(dw) * widths\n",
        "        height = torch.exp(dh) * heights\n",
        "        x_center += dx * widths\n",
        "        y_center += dy * heights\n",
        "\n",
        "        x1 = x_center - 0.5 * width\n",
        "        y1 = y_center - 0.5 * height\n",
        "        x2 = x_center + 0.5 * width\n",
        "        y2 = y_center + 0.5 * height\n",
        "\n",
        "        return torch.stack([x1, y1, x2, y2], dim=1)\n"
      ],
      "metadata": {
        "id": "y1peIUqVu49U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utAcLFRgu42C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1Y6v2oW2v8"
      },
      "source": [
        "### 문제 26. _decode_box_predictions method\n",
        "    - anchor box의 (x,y,w,h)와 model의 box regression 예측값을 이용하여 transform된 box의 위치를 계산합니다.\n",
        "    - 화면에 출력하기 쉽게 하기 위하여 이 값을 (xmin, ymin, xmax, ymax)로 변환하여 반환합니다"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def _decode_box_predictions(anchor_boxes, box_deltas):\n",
        "    \"\"\"\n",
        "    Anchor box와 박스 회귀 예측값을 사용하여 박스의 위치를 계산하고 (xmin, ymin, xmax, ymax) 형태로 반환합니다.\n",
        "\n",
        "    Args:\n",
        "        anchor_boxes (torch.Tensor): Anchor box의 (x, y, w, h) 정보.\n",
        "        box_deltas (torch.Tensor): 박스 회귀 예측값.\n",
        "\n",
        "    Returns:\n",
        "        decoded_boxes (torch.Tensor): 변환된 박스의 위치 정보 (xmin, ymin, xmax, ymax).\n",
        "    \"\"\"\n",
        "    # Anchor box 정보\n",
        "    anchor_x, anchor_y, anchor_w, anchor_h = anchor_boxes.split(1, dim=1)\n",
        "\n",
        "    # 박스 회귀 예측값\n",
        "    delta_x, delta_y, delta_w, delta_h = box_deltas.split(1, dim=1)\n",
        "\n",
        "    # 변환된 박스의 위치 계산\n",
        "    pred_x = delta_x * anchor_w + anchor_x\n",
        "    pred_y = delta_y * anchor_h + anchor_y\n",
        "    pred_w = torch.exp(delta_w) * anchor_w\n",
        "    pred_h = torch.exp(delta_h) * anchor_h\n",
        "\n",
        "    # (xmin, ymin, xmax, ymax) 형태로 변환\n",
        "    xmin = pred_x - 0.5 * pred_w\n",
        "    ymin = pred_y - 0.5 * pred_h\n",
        "    xmax = pred_x + 0.5 * pred_w\n",
        "    ymax = pred_y + 0.5 * pred_h\n",
        "\n",
        "    decoded_boxes = torch.cat((xmin, ymin, xmax, ymax), dim=1)\n",
        "\n",
        "    return decoded_boxes\n"
      ],
      "metadata": {
        "id": "Qz41GDTAvFHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc79lbuuX-Mw"
      },
      "source": [
        "### 문제 27. call method\n",
        "    - box regression 값과 classification 예측값을 받아서, box regreesion 값은 _decode_box_predictions에 넣어주고 classification은 sigmoid를 이용하여 확률값으로 변경해줍니다.\n",
        "    - tf.image.combined_non_max_suppresion을 이용하여 nms를 수행하고 결과를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.ops as ops\n",
        "\n",
        "class DecodePredictions(nn.Module):\n",
        "    def __init__(self, anchors, num_classes, score_threshold=0.05, nms_iou_threshold=0.5, max_detections=100):\n",
        "        super(DecodePredictions, self).__init__()\n",
        "        self.anchors = anchors\n",
        "        self.num_classes = num_classes\n",
        "        self.score_threshold = score_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "    def forward(self, box_deltas, class_probs):\n",
        "        # 박스 변환\n",
        "        decoded_boxes = self._decode_box_predictions(box_deltas)\n",
        "\n",
        "        # 클래스 확률을 확률값으로 변환\n",
        "        class_probs = torch.sigmoid(class_probs)\n",
        "\n",
        "        # NMS 수행\n",
        "        detections = []\n",
        "        for class_id in range(1, self.num_classes):  # 클래스 0은 배경이므로 제외\n",
        "            class_scores = class_probs[:, class_id]\n",
        "            mask = class_scores > self.score_threshold\n",
        "            filtered_boxes = decoded_boxes[mask]\n",
        "            filtered_scores = class_scores[mask]\n",
        "\n",
        "            # Non-Maximum Suppression (NMS)\n",
        "            selected_indices = ops.nms(filtered_boxes, filtered_scores, self.nms_iou_threshold)\n",
        "            selected_boxes = filtered_boxes[selected_indices]\n",
        "            selected_scores = filtered_scores[selected_indices]\n",
        "            selected_class_ids = torch.full((selected_indices.shape[0],), class_id, dtype=torch.int64)\n",
        "\n",
        "            detections.append(torch.cat((selected_boxes, selected_scores.unsqueeze(1), selected_class_ids.unsqueeze(1)), dim=1))\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def _decode_box_predictions(self, box_deltas):\n",
        "        anchors = self.anchors\n",
        "        widths = anchors[:, 2] - anchors[:, 0]\n",
        "        heights = anchors[:, 3] - anchors[:, 1]\n",
        "        x_center = anchors[:, 0] + 0.5 * widths\n",
        "        y_center = anchors[:, 1] + 0.5 * heights\n",
        "\n",
        "        dx, dy, dw, dh = box_deltas.split(1, dim=1)\n",
        "        dx, dy, dw, dh = dx.squeeze(1), dy.squeeze(1), dw.squeeze(1), dh.squeeze(1)\n",
        "\n",
        "        width = torch.exp(dw) * widths\n",
        "        height = torch.exp(dh) * heights\n",
        "        x_center += dx * widths\n",
        "        y_center += dy * heights\n",
        "\n",
        "        x1 = x_center - 0.5 * width\n",
        "        y1 = y_center - 0.5 * height\n",
        "        x2 = x_center + 0.5 * width\n",
        "        y2 = y_center + 0.5 * height\n",
        "\n",
        "        decoded_boxes = torch.stack([x1, y1, x2, y2], dim=1)\n",
        "\n",
        "        return decoded_boxes\n"
      ],
      "metadata": {
        "id": "CxX_p3KMvPt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txeWIQdJM5B3"
      },
      "source": [
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=80,\n",
        "        confidence_threshold=0.05,\n",
        "        nms_iou_threshold=0.5,\n",
        "        max_detections_per_class=100,\n",
        "        max_detections=100,\n",
        "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(DecodePredictions, self).__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.nms_iou_threshold = nms_iou_threshold\n",
        "        self.max_detections_per_class = max_detections_per_class\n",
        "        self.max_detections = max_detections\n",
        "\n",
        "        self._anchor_box = AnchorBox()\n",
        "        self._box_variance = tf.convert_to_tensor(\n",
        "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "        boxes = box_predictions * self._box_variance\n",
        "        boxes = tf.concat(\n",
        "            [\n",
        "                ##### CODE HERE #####\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        boxes_transformed = convert_to_corners(boxes)\n",
        "        return boxes_transformed\n",
        "\n",
        "    def call(self, images, predictions):\n",
        "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
        "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "\n",
        "        ##### CODE HERE #####\n",
        "\n",
        "        return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C97ZdBhJYwiS"
      },
      "source": [
        "### Validation data를 이용하여 결과를 화면에 출력하기\n",
        "\n",
        "    - image를 입력으로 하고 이를 학습된 RetinaNet을 통과시킨 뒤,\n",
        "      위에서 작성한 DecodePredictions를 통과한 결과가 출력이 되는 model을 하나 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RetinaNetInferenceModel(nn.Module):\n",
        "    def __init__(self, retina_net, decode_predictions):\n",
        "        super(RetinaNetInferenceModel, self).__init__()\n",
        "        self.retina_net = retina_net\n",
        "        self.decode_predictions = decode_predictions\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # RetinaNet 모델 통과\n",
        "        box_deltas, class_probs = self.retina_net(inputs)\n",
        "\n",
        "        # DecodePredictions 적용\n",
        "        detections = self.decode_predictions(box_deltas, class_probs)\n",
        "\n",
        "        return detections\n",
        "\n",
        "# RetinaNet 모델과 DecodePredictions을 결합하여 Inference 모델 생성\n",
        "retina_net = YourRetinaNetModel()  # RetinaNet 모델을 여기에 넣으세요\n",
        "decode_predictions = DecodePredictions(anchors, num_classes)  # anchors와 num_classes를 설정하세요\n",
        "inference_model = RetinaNetInferenceModel(retina_net, decode_predictions)\n"
      ],
      "metadata": {
        "id": "Wr0HgFzivqJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHAaXZgiM5MD"
      },
      "source": [
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "predictions = model(image, training=False)\n",
        "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciAlrHkNZMNd"
      },
      "source": [
        "    - image와 bounding box, class name을 출력하기 위한 함수를 다음과 같이 작성합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def draw_boxes(image, boxes, class_names):\n",
        "    \"\"\"\n",
        "    입력 이미지 위에 바운딩 박스와 클래스 이름을 그립니다.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): 입력 이미지.\n",
        "        boxes (list): 바운딩 박스 리스트. 각 박스는 [xmin, ymin, xmax, ymax] 형식의 좌표를 포함합니다.\n",
        "        class_names (list): 클래스 이름 리스트. 각 클래스에 해당하는 이름을 포함합니다.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: 바운딩 박스와 클래스 이름이 그려진 이미지.\n",
        "    \"\"\"\n",
        "    for box, class_name in zip(boxes, class_names):\n",
        "        xmin, ymin, xmax, ymax = map(int, box)\n",
        "        color = (0, 255, 0)  # 바운딩 박스 색상 (여기서는 녹색)\n",
        "        thickness = 2  # 바운딩 박스 두께\n",
        "\n",
        "        # 이미지에 바운딩 박스 그리기\n",
        "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, thickness)\n",
        "\n",
        "        # 클래스 이름 텍스트 설정\n",
        "        text = f\"{class_name}\"\n",
        "        font_scale = 0.7\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        font_thickness = 1\n",
        "        text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
        "\n",
        "        # 텍스트 배경 그리기\n",
        "        bg_color = (0, 0, 0)  # 텍스트 배경 색상 (여기서는 검은색)\n",
        "        text_xmin = xmin\n",
        "        text_ymin = ymin - text_size[1] - 2\n",
        "        text_xmax = text_xmin + text_size[0]\n",
        "        text_ymax = ymin - 2\n",
        "        cv2.rectangle(image, (text_xmin, text_ymin), (text_xmax, text_ymax), bg_color, -1)\n",
        "\n",
        "        # 텍스트 그리기\n",
        "        cv2.putText(image, text, (xmin, ymin - 5), font, font_scale, (0, 0, 255), font_thickness)\n",
        "\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "mnVlZHcPvwdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eYNoWGNNQp_"
      },
      "source": [
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"Visualize Detections\"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQaIZf5EZVOT"
      },
      "source": [
        "    - validation data에서 4개의 batch를 가져와서 각 batch의 첫번째 image에 대한 detection 결과를 다음과 같이 화면에 출력하여 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2c8shVCNFKu"
      },
      "source": [
        "def prepare_image(image):\n",
        "    image, _, ratio = resize_and_pad_image(image)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0), ratio\n",
        "\n",
        "\n",
        "val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n",
        "int2str = dataset_info.features[\"objects\"][\"label\"].int2str\n",
        "\n",
        "for sample in val_dataset.take(4):\n",
        "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
        "    input_image, ratio = prepare_image(image)\n",
        "    detections = inference_model.predict(input_image)\n",
        "    num_detections = detections.valid_detections[0]\n",
        "    class_names = [\n",
        "        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "    ]\n",
        "    visualize_detections(\n",
        "        image,\n",
        "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "        class_names,\n",
        "        detections.nmsed_scores[0][:num_detections],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT_lOk5DO5NS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# validation_dataset에서 4개의 배치 가져오기\n",
        "batch_count = 4\n",
        "for batch_idx, (images, gt_boxes, class_ids) in enumerate(validation_dataset.take(batch_count)):\n",
        "    # 첫 번째 이미지 선택\n",
        "    image = images[0].numpy()\n",
        "\n",
        "    # RetinaNet 모델을 통해 검출 결과 얻기\n",
        "    detections = inference_model(images)\n",
        "\n",
        "    # 검출 결과를 시각화하기 위한 정보 추출\n",
        "    boxes = detections[0][:, :4]  # 바운딩 박스 좌표 (xmin, ymin, xmax, ymax)\n",
        "    scores = detections[0][:, 4]  # 바운딩 박스 점수\n",
        "    class_ids = detections[0][:, 5]  # 클래스 ID\n",
        "\n",
        "    # 점수가 일정 값 이상인 검출 결과만 선택\n",
        "    threshold = 0.5  # 점수 임계값 (조절 가능)\n",
        "    selected_indices = scores >= threshold\n",
        "    selected_boxes = boxes[selected_indices]\n",
        "    selected_class_ids = class_ids[selected_indices]\n",
        "\n",
        "    # 클래스 이름 리스트 (예: COCO 클래스 리스트)\n",
        "    class_names = [\"background\", \"class1\", \"class2\", \"class3\", ...]  # 클래스 이름을 적절히 설정\n",
        "\n",
        "    # 클래스 ID를 클래스 이름으로 변환\n",
        "    selected_class_names = [class_names[int(class_id)] for class_id in selected_class_ids]\n",
        "\n",
        "    # 검출 결과를 이미지에 그리기\n",
        "    result_image = draw_boxes(image.copy(), selected_boxes, selected_class_names)\n",
        "\n",
        "    # 이미지 출력\n",
        "    plt.imshow(result_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Batch {batch_idx + 1}')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}